< 단어 - 원핫인코딩 >
문장을 토크나이즈 과정을 거쳐 단어로 분리하고
고유단어 마다 인덱스를 붙여 	사과=0 바나나=1 맛있다=2
리스트를 만듬				사과는 = [1,0,0] 바나나는 = [0,1,0] 맛있다 = [0,0,1]
+ Word2Vec
문장의 단어는 앞 뒤 단어와 관계가 있기 때문에 CBOW / Skip-gram을 통해 단어를 예측
+ GloVe
전체 문서(Word2Vec은 window라는 단위)을 보고 고유단어를 정리한 다음 각 단어들의 공통출현 관계표를 가짐

< 문장 - 원핫 인코딩 >
단어에 인덱스를 붙여	사과=0 맛있다=2
문장 리스트를 만듬	사과는 맛있다 = [1,0,1]
+문장 임베딩
단어를 임베딩하면서 
문장 전체의 의미를 가진 벡터를 만듬

< 전처리 >
Tokenize		: 고유단어(작은의미의단어)로 분리
Stop Word 제거	: 의미없는 개체 제거
Stemming		: 접두사,접미사 제거

< TF-IDF >
GloVe를 통해 단어들의 임베딩값을 생성하되
단어마다 중요도(TF-IDF)를 나눈다

< LDA >
텍스트마이닝 특화
문서의 Topic을 찾아냄 .. 문서 == N개의 토픽 / 토픽 == M개의 단어
해당 문서/토픽이 어떤 내용인지 확률,비율로 표현 ( Dirichelt분포 )	: 해당 토픽은 햄버거, 치킨, 피자같은 내용이 많으므로 토픽은 음식일 것이다~ 
& 숨어있는 토픽을 가정하고 단어를 토픽에 할당해서 분석

< 워드클라우드 >
빈도가 높은 단어는 크게해주는 시각화툴

< 문장 분류 >
NLP : 문장임베딩을 만들어서 문장의 분류 (평서문, 감탄문 .. )

< 데이터마이닝 >
대용량 데이터 내의 패턴 탐색 후 지식을 추출

< 시계열분석 >
plotlib
주기성 분석

< 데이터웨어하우스, 데이터마트, 메타데이터 >

< 추천시스템 >
콘텐츠 / 협업 / 하이브리드
















