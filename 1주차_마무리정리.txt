데이터 조직이 가치를 만들어내는 방법은 2가지
1. 데이터 기반한 의사 결정하는 것(Decision Science) >> 데이터 분석가
조직구조의 영향을 많이 받는다, 현업부서에서 일하는 사람이 추가적으로 배우려고 한다
2. 데이터 기반한 제품 개선하는 것(Product Science) >> 데이터 과학자
머신러닝 형태로 제품 기능 개선 및 운영을 최적화를 비용을 절감

머신러닝
지도학습, 비지도학습, 강화학습
1. 지도학습
카테고리 예측 classification
연속 숫자 예측 Regression

머신러닝의 발전 > 딥러닝(인공신경망) 재조명 > Generative model 생성 > Gen AI(ChatGPT)

데이터 활용할 때 고려할점, 주의할점 (개인정보)

================나만의 정리================

*데이터
+웹 데이터 : 많은 데이터가 모이는 장소로 선별(신뢰)만 할 수 있다면 해당 데이터를 훈련데이터로 해서 초거대 언어모델(GPT)를 만들 수 있다 .. 하지만 이익을 위해 공공 데이터를 사용하는 것은 문제가 될 수 있다.(저작권) >> 해당 법률이 필요함
+데이터 중요도 품질(신뢰성) > 크기 (Garbage in Garbage out)
**데이터 양, 소스가 증가함으로 보관(data lake), 처리(분산처리, 클라우드) 기술들이 필요해짐
**생길수 있는 문제 : 1.개인정보(GDPR, 생성과 동시에 태깅, 접근권한 제어 및 로깅, 추출과 삭제 자동화) 2.다양한 해석(KPI 정의사전) 3.비슷한 대시보드(데이터 카탈로그, 검색엔진 = data discovery) 4.불분명한 데이터 오너십(메타데이터 관리) 5.메타데이터 부족(중요 데이터 별로 오너 지정, 데이터 리니지 파악, 데이터별 분류) 6 Data Silo : 조직 전반에 메타 데이터 관리/유지, Data mesh
***분산처리 기술 : 데이터 양이 늘어남(빅데이터)에 따라 여러 컴퓨터를 이용하여 부하를 막는다 (하둡,spark,tensorflow)
***클라우드 기술 : 위와 같은 처리를 하되 물리적인 컴퓨터를 이용하지 않고 대행업체를 이용한다 .. AWS > pros) 기회비용 감소, 탄력적 결제, 물리적공간 여유, 효율적인 자원사용, 확장성
***data discovery : 메타 데이터와 데이터셋 오너 관리 + 주기적인 청소작업
***메타데이터 : 데이터에 대한 데이터 ~= 데이터 오너십
***데이터 리니지 : 데이터 흐름의 시각화


*데이터 팀 : 신뢰가능한 데이터들을 바탕으로 부가가치를 생성하여 1.의사결정을 돕고 2.오류를 예측해준다 == 서포팅 역할 
+ 팀 비용이 많이 소모되어 존재가치를 증명하는 것이 중요하다
**구조 : 1. 중앙집중 2.분산 3.하이브리드
*** 중앙집중 : 별도의 조직 존재. 다른 부서로 부터의 request 요청
pros) 전문성,커리어적으로 이득 // cons) 다른 현업부서들의 불만족(우선순위)
*** 분산 : 현업부서별로 존재. 중앙집중구조에서 분산되거나 자생,합병을 통해 생성
pros) 다른 현업부서의 만족 // cons) 팀마다 다른 전략, 적은 데이터 공유, 부서별 중복 데이터 처리 > data mesh 필요
**** data mesh : 조직별로 데이터를 다루되 카탈로그를 작성하여 가시성을 높이고 공유를 하도록 함 (분산데이터 아키텍쳐)
+회사 규모가 커질수록 data warehouse 단일화가 어렵기 때문에 중요도가 높아짐
**** micro service : 비슷한 내용으로 웹개발에서 사용. 목적별로 다수의 서비스를 API형식으로 구현하고 연동
+ micro service-service registry == data mesh-카탈로그
*** 하이브리드 : 중앙 데이터 팀의 파견형식. 가장 이상적
**Agile model 업무 : 짧은 사이클(1~3주) 동안 구체적인 작은 기능을 바로 쓸 수 있을 정도로 구현 .. JIRA툴
+데이터 분석가,엔지니어,과학자 다 같이 회의 (업무복기, 계획)


*데이터 엔지니어 : 대규모를 수집할 때 목적성에 맞도록 시스템을 설계함 (data pipeline/ETL) .. airflow툴 | target = 데이터 팀
+외부 요청에 따른 수동적인 업무 > 요청에 대한 기록(business owner) + 품질관리 + PII 개인정보 분류
**데이터 인프라 : 외/내부 데이터를 수집하여 정제를 거쳐 관계형 데이터베이스(Table) 형태로 data warehouse/lake에 적재 .. 코딩이 중요하고 / 데이터 사용,저장,공유를 지원하는 HW,SW,Networking,Sevice,Policy 전부 포함한다 .. 가장 우선시 되어야한다. 
***data warehouse : 데이터 분석을 위해 목적성을 가지고 수집한 데이터들의 모음 +production database(운영을 위한 최소한의 데이터베이스 .. 빠름) +data lake : 데이터의 양이 증가함에 따라 더 많은 데이터들을 받기위해 구조없이 저장하는 공간(저렴,고용량)

*데이터 분석가 : data warehouse의 데이터를 활용해서 목적에 맞는 중요지표(KPI)를 세워 새로운 테이블을 생성하고 분석하여 대시보드(시각화)를 통해 의사결정에 도움을 줌 (ELT) .. DBT툴 | target = 내부 의사결정권자
+지표를 명확히 생성해서 소통에 문제가 없어야함 (이해성,객관성 .. 지표선정이유 설명)
+가설 수립 후 지표를 생각 .. 한번에 큰 가설을 세우기보단 작은 것부터
+외부 요청에 따른 요청 > 신속한 대응 필요(임기응변)
**data (1.informed/2.driven) decisions : 1.가설의 진행에 데이터를 참고함(도전,혁신) 2.데이터를 바탕으로 미래를 예측(최적화)
**KPI : 보통 숫자로 표현하는 달성하고 자하는 목표, 현재상황 판단의 근거 .. 갯수가 적고, 자세할 수록 좋다(반품을 제외한 매출액)(대시보드도 마찬가지) .. 3A(Accessible(현재상황판단), Actionable(방향성찾기), Auditable(검토가능))
+Cohort analysis를 통한 유의미한(Active) 고객 분리, Bias에 빠지지 않도록 가설 검토
+지표 사전필요(명확한 정의)
+특징 : 가치 표현, 지속적인 가치 창출, 후행지표, 피드백성
***Cohort analysis : 사용자 행동(동일한 특성)을 그룹으로 나눠 수치화 한뒤 분석
***Bias : 1.Survivorship Bias 2.Confirmation Bias = 1.문제가 아닌부분이 문제일 수도 있다 2.문제인 부분을 집중적으로 팜 > 가설 검토
***1.선행지표/2.후행지표 : A 하면 B된다 > 1.A(통제가능) 2.B(통제불가능)
**시각화 대시보드 : 시간의 흐름에 따라 현재상황/KPI에 대한 내용을 보여줌 .. Excel Tableau, Looker, Superset툴
+반복적인 내용은 self service를 사용(Scalability)
**A/B테스트 : 새로운 기능을 테스트할 때 절반으로 나눠서 실험하여 '유의미한(통계적인)' 결과가 나오는지 확인
++필요 스킬셋 : SQL, 파이썬, 스키마,지표정의, DBT툴, 통계적지식, A/B테스트, 대시보드, 비즈니스 도메인 지식
!!한계 : 수동적 업무, 상대적으로 전문성 평가받기 어려움


*데이터 과학자 : 서비스 사용자의 경험 개선과 서비스 운영을 최적화하고 품질을 개선하도록 함 .. 개인화,에러예측, 데이터 모델 생성 | target = 서비스 사용자
+최근은 예측에 대한 근거(데이터모델의 동작과정)를 제시하고 있다
+ML 개발 프레임웍 설계, 데이터 전처리, A/B테스트 디자인
+가설(with 머신러닝) > 훈련데이터 수집(신뢰가능한 데이터) > 모델 빌딩/테스트 > 배포 > A/B테스트 > 결과분석 > 1.전체 launch 2.가설 재수립
**데이터 거버넌스 : 데이터 수집부터 분석, 저장까지하는 데이터 관리의 전체적인 프로세스
+데이터 품질관리에 70%의 시간을 쏟는다 .. 편향성/윤리 문제가 생길수 있기 때문
**머신러닝 : 데이터의 예를 줌으로써 모델이 알아서 분류를 하도록 하는 것(예제를 기준으로 학습(지도,비지도,강화학습)이 가능한 기계)
+사용자의 패턴을 찾아 추천해줌 >> 데이터 품질이 중요, 100% 정확할 필요는 없다
+사람보다 정확한 결과가 나오지만, 책임성에 대한 문제가 있다(법률이 시대를 따라가지 못함) >> 인간의 보조역할로 사용해야한다.
+훈련데이터에 따라 성능이 좌지우지된다(데이터 편향성, 결과의 이유) >> 절대적 신뢰는 안된다
***지도학습 : 1.분류지도학습 2.회귀지도학습 .. 1.명확한 답을 줌(개/고양이) 2.방향성 예측(연속적 숫자)
+레이블/타겟 : 모델이 예측해야할 필드
+Feature Engineering : 주어진 필드로부터 목적에 맞는 새로운 필드를 뽑아내는 것
***비지도학습 : 데이터를 특정 기준으로 그룹핑, 클러스터링하여 새로운 데이터에 대한 예측을 함
+GPT, LM : 문장 뒤에 올 확률이 높은 단어를 예측함
***강화학습 : 보상(reward)을 통해 목적을 달성하기 위해 행동함
**고려할점 : 데이터 모델 개발~배포까지 책임져야하고, 운영에서 생기는 인사이트를 갖고 개선점을 찾는 피드백이 중요. 프레임워크(사용툴) 표준화가 중요하다 .. AWS SageMaker 툴
++필요스킬셋 : 통계지식, 열정/끈기, 코딩, 다양한 경험, 현실적인 접근방법(Agile), 과학적인 접근방법(가설)


*1.ML Ops / 2.Dev Ops : 데이터 엔지니어 + 과학자(1.머신러닝 모델 2.코드)
+새로 생기는 데이터들이 많아 Data Drift가 일어나기 때문에 모델을 주기적으로 자동적으로 재훈련시킴
**Data Drift : 훈련데이터와 실제데이터가 시간의 흐름에 따라 달리 변화함 (유행에 뒤쳐짐) .. 서비스 속도(Latency)에도 영향이 있음
**ML Ops : 데이터 과학자의 데이터 모델을 패키지로 만들어 배포하고 모니터링,logging을 하며 문제 발생시 excalation 프로세스를 수행하여 문제 재발방지까지하는 서비스부터 운영(CT)까지의 역할을 한다 .. 언제 발생할지 모르기 때문에 on-call 프로세스를 따른다
***on-call 프로세스 : 5분대기조
***CT : Data Drift가 생기면 훈련,테스트,배포까지 자동화하는것
**Dev Ops : 위와 동일하지만 대상이 모델이 아닌 개발자의 코드 이다.


*ELT / ETL
	ELT		ETL
who	분석가		엔지니어
from	data warehouse	내/외부데이터
tool	DBT		Airflow
목적	의사결정		데이터 수집


*데이터 문해력 : 데이터를 이해하고 활용할 수 있는 능력 .. 모든 직군에 중요성이 확대되고 있음

*딥러닝 : input에서 예상되는 output을 흉내내기 위해 hidden layer에서 가중치를 정하는 것 .. 1.Discriminative model 2.Generative model(Gen AI) : 1.정답이 있는 채로 학습(개/고양이 분류 .. 지도학습) 2.패턴 학습을 통해 새로운 데이터 생성(개 이미지 생성 .. 비지도학습) >> 통계적 틍성 이용
**Gen AI : 학습된 컨텐츠로 새로운 컨텐츠를 만들어내는 딥러닝 기술 (from 파운데이션 모델) .. GPT
+LLM⊂Gen AI⊂딥러닝
+한계 : 헛소리 .. 모델의 output이 부정확하거나 틀릴 수도(데이터 불충분, 최신성 부족, 데이터 품질이슈, output에 대한 정확성부족) > 사실확인 필요 .. prompt 디자인이 중요해짐
+prompt : 모델에게 어떤 행동을 해야하는지 자연어로 설명하고 원하는 결과물을 출력할 수 있도록 하는 방식 .. 보통 사람들과의 대화방식 (role, task, objective, tome, restriction, format)역할을 정해주면 좋다
+독창적 아이디어, 보조역할로 사용하면 좋지만, 전적으로 판단을 맡겨버리거나 빠른 시대변화를 법제가 따라가지 못하므로 여러가지 문제(저작권)이 생길 수 있다.
***파운데이션 모델(트랜스포머 모델) : 많은 돈과 시간, 경험을 사용하여 광범위한 데이터셋을 학습시킴(Pretrained) .. 특정 토픽보단 다양한 토픽에서 적용이 가능
+Multi modal(ity) foundation model : 하나의 모델이 여러 형태의 미디어를 서포트하는 것
+Fine-Tuning : Pretrained 데이터(파운데이션모델)에서 도메인의 특성을 살린(특정 토픽) 데이터를 학습시키자 .. 목적 데이터에 가중치를 넣고 재훈련 시킴. 단 데이터(pretrained) 수정은 하지 않음
***GPT : OpenAI에서 만든 초거대 언어모델 .. 1.word completion 2.code completion
+짧은 기간 내에 비약적인 성장 (GPT3 > GPT4 > GPT4 Turbo)
+ChatGPT : GPT를 prompt와 Fine-Tuning을 이용해서 챗봇형식을 만들되 피드백(RLHF)을 받으며 강화학습을 진행(재훈련)한 모델
+RLHF : 대화예제(정답)을 기준으로 Fine-Tuning > ChatGPT응 답에 따른 피드백을 통한 재훈련(강화훈련) > 검토 트레이닝(훈련의 정당성)
+발전 : No coding형식(GPT Builder) > 파일을 직접 업로드해서 Fine-Tuning할 수 있다.

*개인정보 보호 : 개인정보를 동의없이 저장, 노출, 배포 .. 개인정보는 불필요하게 저장되면 안된다(필요한 것만 저장) .. 필요한 사람이 필요할때 저장하고 모든 엑세스를 워딩과 기록을 해서 나중에 감사, 오딧(Audit)할 수 있게 해야한다.
+GDPR : 유럽연합의 개인정보보호 법령(데이터 카탈로그, 거버넌스).. 필수 .. CPRA(CCPA), HIPPA
큰회사들은 독자적인 시스템을 만들어 대응
+EHR/EMR, ePHI .. 최신 데이터와 부족하고 구조화된 데이터가 아니라 의사마다 다름 .. 수기기록을 이미지화 해서 받는 경우도 있음





airflow / DBT 툴 사용하기
Feature Engineering == ELT 인거지?


