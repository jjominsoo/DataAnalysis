검증
모델의 학습이 잘 진행됐는지 판단하는 평가
너무 쉬우면 일반화가 어렵고 너무 어려우면 과적합이 일어날 수 있음

교차검증
쉬운데이터, 데이터양 부족을 감수하면서 일반화 능력을 판단하는 평가
전체 데이터를 여러개의 하위데이터로 나누고 
그 하위데이터들의 조합은 다른 방법의 훈련과 검증을 진행하고 평균을 냄

++일반화 추정, 데이터활용, 과적합 방지
---시간 오래걸림

1. K-Fold CV	: 전체를 K개의 폴드로 나누고 각 폴드를 검증데이터셋하고 나머지(K-1개의 폴드)는 학습데이터
학습방법마다 K번의 검증을 돌리면 점수가 나옴 >> 그 점수가 높은 학습방법이 좋은 방법이라고 여김
전체 데이터를 위에서 구한 방법으로 학습시키면 모델 끝 ( 실제데이터 적용가능 )

2. 계층적 교차검증	: 위와 똑같이 나누되 각 폴드의 분포가 전체 데이터 분포랑 동일하게 나눔 ( class balancing )
K-Fold에서 나올수 있는 폴드마다의 불균형(편향) 해결

3. LOOCV		: K가 데이터 전체수 .. 하나의 데이터 빼고 나머지를 모두 학습

>>>>
Class별 불균형이 심하다 ( 1 : 80% 2 : 20% ) 	: 2.계층적 교차검증
아니면					: 1.K-Fold CV




성능평가
모델의 성능을 객관적 지표(metric)로 측정, 비교 == metric선정이 중요 (목적에 맞고, 해석이 잘되고 .. )
비지도학습, 고객만족점수 등 적절한 metric이 존재하지 않을 경우 시각화도 좋음

Metric
분류	: 정확도, 정밀도, 재현율, F1 score, 혼동행렬(2x2 정답/예측행렬 그래프)
회귀	: MSE
군집화	: SSE, 실루엣계수
이상치	: 위의 분류 metric



1. 분류문제 성능추가 Metric

+ROC curve
이진분류(T/F)에 쓰이는 모델성능측정 도구 >> T/F를 나누는 기준(임계값)의 변화에 따른 성능을 시각화한 그래프
+AUC 점수
ROC curve 밑의 영역 .. 분류가 잘될수록 (T/F가 명확히 나눠질수록) 밑의 면적이 커진다 == 좋은 분류다
www.navan.name/roc/
즉.
ROC curve의 꼭지점을 통해 T/F분류의 기준 %를 알 수 있고
AUC 점수를 통해 분류가 잘된건지를 알 수 있다.



2. 회귀문제 성능추가 Metric

+R^2 == 결정계수
회귀선(경향선)과 데이터가 얼마나 퍼져있는지(데이터변동성) 평가해주는 metric

1 - ( (모델예측값<>실제값 거리)^2의 합 	/ (평균<>실제값 거리)^2의 합 )
       모델이 실제 설명하고 있는 변동성량	/ 모델이 예측해야하는 데이터들의 전체 변동성 양

1	: 모든 포인트가 회귀선에 있음
0 	: 모델의 평균값 예측
0~1	: 일부 변동성 .. 클수록 설명력이 높다 but 과적합 주의

