<선형회귀>

독립변수(서로 영향이 없)가 파라미터 값만큼 일정한 비율로 종속변수에 영향을 미치는 관계 = 덧셈식 ( y = w1x1 + w2x2 + .. )
+ 입력 데이터의 해석에 따라 선형/비선형이 없다 ( x^2를 새로운 x3로 생각해도 됨)
>> 데이터 분석 시 일단 선형이라고 가정함 == 여러 데이터를 보고 최적의 파라미터를 찾아야함
+ 독립성(낮은 상관관계)가 중요 (다중공신성)
>> 정확도, 신뢰도, 해석력에 문제가 생길 수 있음 = 상관관계가 높으면 다른 모델 선택

비용함수 > '평균 제곱 오차' 이용 ( 손실의 부호 통일 ) == J(w) = 시그마(y-y2)^2 ( (정답 - 예측값)제곱의 총합 )
최소값을 찾아야함 min J(w) >> how? 1)정규방정식 2)경사하강법

1) 정규방정식
미분값이 0이 되는 파라미터를 직접 계산
특성(feature)에 따라 시간 복잡도가 생김 >> 데이터 수가 특성보다 많은 경우 사용 ( O(p^3) )
<<코딩>>
1. 데이터가 정규분포를 띄어야한다. ( np.random.normal(0,3,num_data) )
2. 비교를 위한 정답 파라미터 정의 ( w0, w1 )
3. x = 데이터의 범위만큼 일정간격으로 나눔 ( x축의 최대값을 표시 )
3+. y = w0 + w1*x + noise 꼴로 데이터가 나와야하므로 (선형) x축의 범위를 지정 ( np.linespace(0,20,num_data) == 0 <x<20까지의 범위를 보겠다.
4. 얻은 x, y값으로 Dataframe 생성 ( pd.DataFrame({'x':x, 'y':y})
4+. plt.scatter(data['x'], data['y'])로 데이터 분포를 미리 확인해보자
5. 위에서 얻은 x와 y는 ( num_data, 1 )의 배열을 갖고 있다. >> OLS수식을 적용하기 위해 x의 배열변경이 필요하다.
x = (100,1) / y = (100,1)
5+. np.stack() 과 np.ones_like()를 통해 [ [1,x0] [1,x1] [1,x2] .. [1,xnum_data] ] 를 만듬 ( axis =1 주의 )
X = (100,2)
6. 최소제곱법[OLS]을 이용해 데이터를 기반으로 한 파라미터(w0' w1')를 계산한다.
def calc_OLS(X, y):
    w_ols = np.linalg.inv(X.T @ X) @ X.T @ y
    return w_ols
>> w_ols[0] = w0' / w_ols[1] = w1'
6+. y' = w0' + w1'*x 로 새로운 경향선을 만들고 비교한다
plt.plot(x, y) plt.plot(x,y')


2) 경사하강법
점진적으로 파라미터 조정 >> 데이터가 적을수록 유리
임시로 파라미터를 조정해서 기울기를 계산해서 0에 가까워지도록 조정
! 시작점 선택 + 파라미터 변화지정(학습률) 임계점 설정
++ 확률적 경사 하강법
데이터가 많으면 불리하기 때문에 데이터를 샘플링(소분)해서 여러개의 그룹으로 나눠서 진행

<<코딩>>
1. 학습률, 에포치, 시작지점 지정(랜덤) ( x축만큼 얼마나 이동할지, 몇번 이동할지, 어디서 시작할지 )
1+. 학습률, 에포치를 적절히 조정해야 값이 나온다.
2. [SGD] 함수를 이용해서 새로운 파라미터값 도출
def calc_SGD(x, y, lr, epochs, init_params):
    for i in range(epochs):
        y_pred = init_params[0] + init_params[1] * x
        error = y_pred - y

        gradient = 2 * X.T @ (X @ init_params - y.reshape(-1, 1)) # reshape : vector를 2차원 matrix로 변환

        init_params -= lr * gradient
    return init_params
>> w_sgd[0][0] = w0'' / [1][0] = w1''

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
기본적으로 두 방법을 모두 사용하고 해를 비교
전혀 상관없는 해들 표출한다면 두 데이터의 feature가 상관관계가 있는 것 (다중공선성)

1. 1)정규방정식에서 X.T(역행렬)이 존재하지 않음 or 특이행렬

SVD-OLS
역행렬을 못구하는 X를 치환해서 진행
O(n * p^2) ) >> 정규방정식에서는 데이터 양(n) 상관없이 feature 수(p)만 상관 있었지만 이젠 아니다.
def calc_SVDOLS(X, y):
    # svd를 적용해 U, SIGMA, V^T(Vt)를 구함
    U, s, Vt = np.linalg.svd(X, full_matrices=False)
    SIGMA = np.diag(s)

    # SVD-OLS 수식 적용
    w_svdols = Vt.T @ np.linalg.inv(SIGMA) @ U.T @ y
    return w_svdols


++
결국 내장함수가 있다
from sklearn.linear_model import LinearRegression
linear_reg = LinearRegression()
linear_reg.fit(X, y)
>> 다중공신성 있는 데이터도 결과는 내줌 ( 정답은 아님 )

y = w0 + w1*x1 + w2*x2 + ... + noise
>> linear_reg.intercept_ = w0 / linear_reg.coef_[1] = w1 / linear_reg.coef_[2] = w2 ...


2. 과적합(훈련데이터 맞춤) >> 규제를 통해 파라미터값 조정(보통 낮추는 방향)
규제 : 어차피 비용함수의 값을 줄일껀데 파라미터값도 같이 줄여보자 >> J = J + sum(w) 파라미터 값의 합
2-1) 라쏘 회귀
alpha * (파라미터의 절대값)을 더함 ( w0 + w1 + .. )
>> 계산하다보면 wk=0 인 값이 나옴 (xk는 계산에 상관이 없음) == 해당 feature는 전혀 결과와 상관이 없음 >> 제거 가능

2-2) 릿지 회귀
alpha * (파라미터의 제곱값의 루트)를 더함 ( 루트( w0^2 + w1^2 .. ) )
>> 모든 feature를 사용함

내장함수 
from sklearn.linear_model import Lasso, Ridge
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
ridge_reg = Ridge(alpha=0.1)
ridge_reg.fit(X, y)