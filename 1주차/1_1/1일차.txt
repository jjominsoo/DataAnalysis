분산처리 : 빅데이터를 처리하기 위해(하둡) 한 컴퓨터가 아닌 여러 컴퓨터를 이용해 부하를 막는다
++이후 클라우드라는 기술을 통해 물리적인 컴퓨터를 직접 구매하지 않고도 대행업체를 통해 같은 기능을 구현할 수 있게 됐다. >> 인프라[경제활동의 기반을 형성하는 시설] 구축완료

중요도 : 품질 > 크기

웹 : 검색하는 데이터(검색창에 적음)와 사용하는 데이터(검색결과 중 클릭한 데이터)에서도 정보를 얻는다
웹에서 데이터들을 선별할 수 있다면(올바른 정보) 초거대 언어모델(GPT)을 만들 수 있다. == 웹이 거대모델의 훈련데이터가 됨 .. 하지만 모델을 만든 기업의 이익을 위해서 공공의 데이터를 사용하는 것은 문제가 될 수도 있음

데이터 팀의 미션 : 서포팅 능력 == 의사결정을 돕고 최적화 해주고 오류 예측해주고 등등
'신뢰'가능한 데이터를 통해 '부가가치'생성
생각보다 비용이 많이드므로 리턴이 확실하지 않다. >> 존재가치를 증명하는 것이 필요

대표적 업무 및 발전성
!! 모든 데이터는 고품질(신뢰가능)이어야 한다.
1. 데이터 인프라 구축
데이터 파이프라인 + ETL(extract, transform, load) > 데이터인프라에서 정제를 거쳐 data warehouse에 적재함 .. 관계형 데이터베이스라서 Table형태로 저장
코딩이 중요 / ETL들의 작성,관리를 위해 airflow 오픈소스 활용 / 파이썬 주로 사용하되 pandas 라이브러리는 한계가 있으므로(단일컴퓨터) spark같은 빅데이터를 처리하기 위한 기술을 사용
+데이터 웨어하우스
production database	회사 서비스 운영을 위한 최소한의 정보를 가진 데이터베이스 > 빠른 처리속도
data warehouse		데이터 분석을 위한 별개의 데이터베이스 > 특정 목적성을 가지고 데이터 수집 ( sql을 이용하기 때문에 구조화된 데이터 처리 최적 )
data lake			아직 활용할 수 없는 데이터(로그데이터), 비구조화 데이터 등 정제과정없이 저장하는 공간 ( 저렴하고 용량이 큼 )
>> 데이터를 data lake에 저장하고 이후 정제과정을 거쳐 data warehouse에 저장
.. 데이터 엔지니어

2. 의사결정 도움(Decision Science)
data warehouse에 적재된 데이터(테이블)를 join해서 자신들이 쓸 수 있도록 새로운 테이블을 생성하고 분석을 진행(ELT) .. DBT툴을 이용함
data informed decisions	: 가설이나 방향으로 진행함에 있어 필요한 데이터들을 참고함	(도전)
data driven decisions	: 여태 나온 데이터들을 바탕으로 앞으로를 결정함		(최적화)
+순서
데이터에서 KPI(중요지표) / 일반지표(참고지표) 정의 >> 지표들을 통해 해석할 수 있도록 시각화(대시보드) + 리포트 작성
* KPI (key performance indicator) : 달성하고자 하는 목표. 보통은 숫자. 명확한 정의 필요(반품을제외한매출액). 적을수록 좋음. 잘 정의될 수록 현재 상황을 파악하기 쉬움. OKR(objects and key results)같이 목표와 결과값을 확인하기 위한 지표
보통 지표를 통해 현재상황에 대한 판단(Accessible)과 이에 맞춰 방향성을 찾고(Actionable) 지표값을 검토할 수 있어야한다.(Auditable) .. 3A
* 시각화 대시보드 : Accessible과정에서 시간의 흐름 순서에 따라 활용
.. 데이터 분석가 | target : 내부 의사결정권자

3. 기능 개선(Product Science)
사용자 경험 개선 / 서비스 운영 최적화(비용절감) / 품질 개선
알고리즘 > 개인화 / 최적화 > 에러예측, 최소화
수집한 데이터에서 패턴을 찾아 모델링을 하고 이에 맞는 데이터를 또 수집한다. >> 선순환구조
.. 데이터 과학자 | target : 서비스 사용자

클라우드
컴퓨팅 자원을 대행업자에게 잠시 임대료를 내면서 사용 .. No Provisioning / Pay As You Go
+장점
시간 단축 == 기회비용 감소 / 탄력적인 결제 == 초기투자비용 적음 / 물리적공간확보 / 효율적인 자원 사용(Peak time에 집중되는 자원사용량 관리 .. capacity planning) / 글로벌 확장성 
* AWS : 아마존 웹 서비스 

데이터 스킬셋
1._데이터_문해력이란.pdf 38슬라이드 그림 참고
우측(데이터 분석가)
+ A/B 테스트 : 가설을 기반으로 새로운 기능 제안을 하되 이 기능이 의미있는 결과를 가져오는지 확인 (data informed decisions)
A(50%..control): 기존 기능 B(50%..test) : 새로운 기능 >> 지표를 기준으로 두개의 차이가 통계적으로 유의미해야한다.
좌측(데이터 엔지니어)
중앙(데이터 과학자)
데이터전처리 A/B테스트 디자인
end to end 프로세스(ML 개발 프레임웍)
좌측+중앙 (ML Ops)

+데이터 엔지니어
보통 외부요청이 오면 그에 따라 데이터를 가져옴 >> 요청이 많아지면? 기록 필요 == 비지니스 오너 >> 품질관리
PII(personal identifiable information) 개인정보 분류
+데이터 분석가
지표정리 >> ELT
* ETL / ELT
ETL	: 데이터 엔지니어가 주로 하고 외/내부 데이터를 처리해서 data warehouse/lake에 가져오는 것 .. 데이터 엔지니어
ELT	: data warehouse/lake에서 목적에 따라 데이터를 조합해서 새로운 데이터를 가져옴 .. 데이터 분석가, DBT툴 사용
+데이터 과학자
패턴을 통해 미래를 예측(개인화)  .. 예측에 대한 근거를 제공하는지(모델의 동작)에 대한 요구가 늘어나고 있다
garbage in garbage out, 개인정보? >> 신뢰가능한 훈련데이터만을 가져와야한다
* 가설에 따른 훈련 데이터 수집 > 

데이터 문해력
데이터를 이해하고 활용할 수 있는 능력 >> 모든 부서로 중요성이 확대되고 있다.

데이터 거버넌스
데이터 관리의 프로세스(품질보장 ,문제발생시 영향 최소화)

주의할점 
1. 데이터 분석가는 상대적으로 몸값이 높기 때문에 결과로 보여주지 못하면 짤릴 수 있다.
2. 보통 업무는 외부 팀들로부터 들어오기 때문에 그들의 기대를 만족시킬 수 있는 결과가 빨리(단기적) 나오게 해야한다.
3. 지표를 잘 생성해서 다른 팀들도 신뢰,이해가 가능해야한다.
4. 인프라가 중요하기 때문에 업무와 동시에 진행하도록 한다.(풀스택) 
> 클라우드? 직접? == 클라우드 위에서 인프라 구축 / 배치? 실시간? == 배치로 수집하여 실시간으로 보도화함. 하지만 실시간 기술들이 늘어나고 있어 그쪽으로 기우는중 .. 배치 : 리소스를 안쓰는 시간대(하루 끝, 야간)에 처리하는 방식
5. 데이터 품질관리가 매우 중요 ( 데이터 클린업과정이 모델링 시간중 70%를 차지함 ) + 데이터 모니터링 + 문제발생 시 애프터케어 >> 데이터 거버넌스
6. 가설 수립 후 지표를 생각 .. 해당 지표의 객관성과 선정이유를 설명할 수 있어야한다 + 가설을 세우는 것 자체로도 인사이트를 키우는데 도움이 됨
7. 작은 것부터 간단한 솔루션부터 시작 > 딥러닝을 고집할 필요없다 + 한번에 모델만드는 것보다 훨씬 빠르고 성공률도 높음






